{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1d6d18b",
   "metadata": {},
   "source": [
    "# Week 6: File ingestion and schema validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936af624",
   "metadata": {},
   "source": [
    "Take any csv/text file of 2+ GB of your choice. --- (You can do this assignment on Google colab)\n",
    "\n",
    "Read the file ( Present approach of reading the file )\n",
    "\n",
    "Try different methods of file reading eg: Dask, Modin, Ray, pandas and present your findings in term of computational efficiency\n",
    "\n",
    "Perform basic validation on data columns : eg: remove special character , white spaces from the col name\n",
    "\n",
    "As you already know the schema hence create a YAML file and write the column name in YAML file. --define separator of read and write file, column name in YAML\n",
    "\n",
    "Validate number of columns and column name of ingested file with YAML.\n",
    "\n",
    "Write the file in pipe separated text file (|) in gz format.\n",
    "\n",
    "Create a summary of the file:\n",
    "\n",
    "Total number of rows,\n",
    "\n",
    "total number of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83e2e7b",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c48d30",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "442c181b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datatable in c:\\users\\hp\\downloads\\anaconda\\lib\\site-packages (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "923b563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cae260af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting testutility.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile testutility.py\n",
    "import logging\n",
    "import os\n",
    "import subprocess\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import datetime \n",
    "import gc\n",
    "import re\n",
    "\n",
    "\n",
    "################\n",
    "# File Reading #\n",
    "################\n",
    "\n",
    "def read_config_file(filepath):\n",
    "    with open(filepath, 'r') as stream:\n",
    "        try:\n",
    "            return yaml.safe_load(stream)\n",
    "        except yaml.YAMLError as exc:\n",
    "            logging.error(exc)\n",
    "\n",
    "\n",
    "def replacer(string, char):\n",
    "    pattern = char + '{2,}'\n",
    "    string = re.sub(pattern, char, string) \n",
    "    return string\n",
    "\n",
    "def col_header_val(df,table_config):\n",
    "    '''\n",
    "    replace whitespaces in the column\n",
    "    and standardized column names\n",
    "    '''\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df.columns = df.columns.str.replace('[^\\w]','_',regex=True)\n",
    "    df.columns = list(map(lambda x: x.strip('_'), list(df.columns)))\n",
    "    df.columns = list(map(lambda x: replacer(x,'_'), list(df.columns)))\n",
    "    expected_col = list(map(lambda x: x.lower(),  table_config['columns']))\n",
    "    expected_col.sort()\n",
    "    df.columns =list(map(lambda x: x.lower(), list(df.columns)))\n",
    "    df = df.reindex(sorted(df.columns), axis=1)\n",
    "    if len(df.columns) == len(expected_col) and list(expected_col)  == list(df.columns):\n",
    "        print(\"column name and column length validation passed\")\n",
    "        return 1\n",
    "    else:\n",
    "        print(\"column name and column length validation failed\")\n",
    "        mismatched_columns_file = list(set(df.columns).difference(expected_col))\n",
    "        print(\"Following File columns are not in the YAML file\",mismatched_columns_file)\n",
    "        missing_YAML_file = list(set(expected_col).difference(df.columns))\n",
    "        print(\"Following YAML columns are not in the file uploaded\",missing_YAML_file)\n",
    "        logging.info(f'df columns: {df.columns}')\n",
    "        logging.info(f'expected columns: {expected_col}')\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe795d5",
   "metadata": {},
   "source": [
    "# 1) Trying different methods of file reading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ada7013",
   "metadata": {},
   "source": [
    "### Time it took with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "64bec368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it took to read the file using Dask:  0.023914813995361328 sec\n"
     ]
    }
   ],
   "source": [
    "from dask import dataframe as dd\n",
    "start = time.time()\n",
    "dask_df = dd.read_csv('data.csv')\n",
    "end = time.time()\n",
    "print(\"Time it took to read the file using Dask: \",(end-start),\"sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617ce89f",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ea0029",
   "metadata": {},
   "source": [
    "### Time it took with Panads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "56a1317d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it took to read the file using panads:  87.40052461624146 sec\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "start = time.time()\n",
    "pd_df = pd.read_csv('data.csv')\n",
    "end = time.time()\n",
    "print(\"Time it took to read the file using panads: \",(end-start),\"sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d9e488",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3a5227",
   "metadata": {},
   "source": [
    "### Time it took with Datatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f8dc49a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it took to read the file using Datatable: 35.583712577819824 sec\n"
     ]
    }
   ],
   "source": [
    "import datatable as dt\n",
    "\n",
    "start = time.time()\n",
    "df_dt = dt.fread('data.csv')\n",
    "end = time.time()\n",
    "print(\"Time it took to read the file using Datatable:\", (end-start),\"sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023a34f4",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0761f0a9",
   "metadata": {},
   "source": [
    "#### Dask was the fastest one by a lot of time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9e2618",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2effe39",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b7d3a9",
   "metadata": {},
   "source": [
    "# 2) Removeing special character , white spaces etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d186d100",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_time</th>\n",
       "      <th>event_type</th>\n",
       "      <th>product_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>category_code</th>\n",
       "      <th>brand</th>\n",
       "      <th>price</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_session</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-11-01 00:00:00 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>1003461</td>\n",
       "      <td>2053013555631882655</td>\n",
       "      <td>electronics.smartphone</td>\n",
       "      <td>xiaomi</td>\n",
       "      <td>489.07</td>\n",
       "      <td>520088904</td>\n",
       "      <td>4d3b30da-a5e4-49df-b1a8-ba5943f1dd33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-11-01 00:00:00 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>5000088</td>\n",
       "      <td>2053013566100866035</td>\n",
       "      <td>appliances.sewing_machine</td>\n",
       "      <td>janome</td>\n",
       "      <td>293.65</td>\n",
       "      <td>530496790</td>\n",
       "      <td>8e5f4f83-366c-4f70-860e-ca7417414283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-11-01 00:00:01 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>17302664</td>\n",
       "      <td>2053013553853497655</td>\n",
       "      <td>NaN</td>\n",
       "      <td>creed</td>\n",
       "      <td>28.31</td>\n",
       "      <td>561587266</td>\n",
       "      <td>755422e7-9040-477b-9bd2-6a6e8fd97387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-11-01 00:00:01 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>3601530</td>\n",
       "      <td>2053013563810775923</td>\n",
       "      <td>appliances.kitchen.washer</td>\n",
       "      <td>lg</td>\n",
       "      <td>712.87</td>\n",
       "      <td>518085591</td>\n",
       "      <td>3bfb58cd-7892-48cc-8020-2f17e6de6e7f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-11-01 00:00:01 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>1004775</td>\n",
       "      <td>2053013555631882655</td>\n",
       "      <td>electronics.smartphone</td>\n",
       "      <td>xiaomi</td>\n",
       "      <td>183.27</td>\n",
       "      <td>558856683</td>\n",
       "      <td>313628f1-68b8-460d-84f6-cec7a8796ef2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                event_time event_type  product_id          category_id  \\\n",
       "0  2019-11-01 00:00:00 UTC       view     1003461  2053013555631882655   \n",
       "1  2019-11-01 00:00:00 UTC       view     5000088  2053013566100866035   \n",
       "2  2019-11-01 00:00:01 UTC       view    17302664  2053013553853497655   \n",
       "3  2019-11-01 00:00:01 UTC       view     3601530  2053013563810775923   \n",
       "4  2019-11-01 00:00:01 UTC       view     1004775  2053013555631882655   \n",
       "\n",
       "               category_code   brand   price    user_id  \\\n",
       "0     electronics.smartphone  xiaomi  489.07  520088904   \n",
       "1  appliances.sewing_machine  janome  293.65  530496790   \n",
       "2                        NaN   creed   28.31  561587266   \n",
       "3  appliances.kitchen.washer      lg  712.87  518085591   \n",
       "4     electronics.smartphone  xiaomi  183.27  558856683   \n",
       "\n",
       "                           user_session  \n",
       "0  4d3b30da-a5e4-49df-b1a8-ba5943f1dd33  \n",
       "1  8e5f4f83-366c-4f70-860e-ca7417414283  \n",
       "2  755422e7-9040-477b-9bd2-6a6e8fd97387  \n",
       "3  3bfb58cd-7892-48cc-8020-2f17e6de6e7f  \n",
       "4  313628f1-68b8-460d-84f6-cec7a8796ef2  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dd.read_csv('data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e38df73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['event_time', 'event_type', 'product_id', 'category_id',\n",
       "       'category_code', 'brand', 'price', 'user_id', 'user_session'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a884448d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-006e030d8d0a>:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df.columns=df.columns.str.replace('[#,@,&,_]','')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['eventtime', 'eventtype', 'productid', 'categoryid', 'categorycode',\n",
       "       'brand', 'price', 'userid', 'usersession'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns=df.columns.str.replace('[#,@,&,_]','')\n",
    "df.columns = df.columns.str.replace(' ', '')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aff0f6",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a289b1",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fa9570",
   "metadata": {},
   "source": [
    "# 3) YAML file Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e0ba0493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting file.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile file.yaml\n",
    "file_type: csv\n",
    "file_name: data\n",
    "inbound_delimiter: \",\"\n",
    "columns: \n",
    "    - event_time\n",
    "    - event_type\n",
    "    - product_id\n",
    "    - category_id\n",
    "    - category_code\n",
    "    - brand\n",
    "    - price\n",
    "    - user_id\n",
    "    - user_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7eee4256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_type': 'csv',\n",
       " 'file_name': 'data',\n",
       " 'inbound_delimiter': ',',\n",
       " 'columns': ['event_time',\n",
       "  'event_type',\n",
       "  'product_id',\n",
       "  'category_id',\n",
       "  'category_code',\n",
       "  'brand',\n",
       "  'price',\n",
       "  'user_id',\n",
       "  'user_session']}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import testutility as util\n",
    "config_data = util.read_config_file(\"file.yaml\")\n",
    "config_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8571d17a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_time</th>\n",
       "      <th>event_type</th>\n",
       "      <th>product_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>category_code</th>\n",
       "      <th>brand</th>\n",
       "      <th>price</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_session</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-11-01 00:00:00 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>1003461</td>\n",
       "      <td>2053013555631882655</td>\n",
       "      <td>electronics.smartphone</td>\n",
       "      <td>xiaomi</td>\n",
       "      <td>489.07</td>\n",
       "      <td>520088904</td>\n",
       "      <td>4d3b30da-a5e4-49df-b1a8-ba5943f1dd33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-11-01 00:00:00 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>5000088</td>\n",
       "      <td>2053013566100866035</td>\n",
       "      <td>appliances.sewing_machine</td>\n",
       "      <td>janome</td>\n",
       "      <td>293.65</td>\n",
       "      <td>530496790</td>\n",
       "      <td>8e5f4f83-366c-4f70-860e-ca7417414283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-11-01 00:00:01 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>17302664</td>\n",
       "      <td>2053013553853497655</td>\n",
       "      <td>NaN</td>\n",
       "      <td>creed</td>\n",
       "      <td>28.31</td>\n",
       "      <td>561587266</td>\n",
       "      <td>755422e7-9040-477b-9bd2-6a6e8fd97387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-11-01 00:00:01 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>3601530</td>\n",
       "      <td>2053013563810775923</td>\n",
       "      <td>appliances.kitchen.washer</td>\n",
       "      <td>lg</td>\n",
       "      <td>712.87</td>\n",
       "      <td>518085591</td>\n",
       "      <td>3bfb58cd-7892-48cc-8020-2f17e6de6e7f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-11-01 00:00:01 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>1004775</td>\n",
       "      <td>2053013555631882655</td>\n",
       "      <td>electronics.smartphone</td>\n",
       "      <td>xiaomi</td>\n",
       "      <td>183.27</td>\n",
       "      <td>558856683</td>\n",
       "      <td>313628f1-68b8-460d-84f6-cec7a8796ef2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                event_time event_type  product_id          category_id  \\\n",
       "0  2019-11-01 00:00:00 UTC       view     1003461  2053013555631882655   \n",
       "1  2019-11-01 00:00:00 UTC       view     5000088  2053013566100866035   \n",
       "2  2019-11-01 00:00:01 UTC       view    17302664  2053013553853497655   \n",
       "3  2019-11-01 00:00:01 UTC       view     3601530  2053013563810775923   \n",
       "4  2019-11-01 00:00:01 UTC       view     1004775  2053013555631882655   \n",
       "\n",
       "               category_code   brand   price    user_id  \\\n",
       "0     electronics.smartphone  xiaomi  489.07  520088904   \n",
       "1  appliances.sewing_machine  janome  293.65  530496790   \n",
       "2                        NaN   creed   28.31  561587266   \n",
       "3  appliances.kitchen.washer      lg  712.87  518085591   \n",
       "4     electronics.smartphone  xiaomi  183.27  558856683   \n",
       "\n",
       "                           user_session  \n",
       "0  4d3b30da-a5e4-49df-b1a8-ba5943f1dd33  \n",
       "1  8e5f4f83-366c-4f70-860e-ca7417414283  \n",
       "2  755422e7-9040-477b-9bd2-6a6e8fd97387  \n",
       "3  3bfb58cd-7892-48cc-8020-2f17e6de6e7f  \n",
       "4  313628f1-68b8-460d-84f6-cec7a8796ef2  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the file using config file\n",
    "#Using the dynamic way of reading the file so it would be flexible \n",
    "file_type = config_data['file_type']\n",
    "source_file = \"./\" + config_data['file_name'] + f'.{file_type}'\n",
    "dataframe = pd.read_csv(source_file,config_data['inbound_delimiter'])\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3c700d",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c262dce1",
   "metadata": {},
   "source": [
    "# 4) Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8713c439",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column name and column length validation passed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "util.col_header_val(dataframe ,config_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0bdf725c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns of files are: Index(['event_time', 'event_type', 'product_id', 'category_id',\n",
      "       'category_code', 'brand', 'price', 'user_id', 'user_session'],\n",
      "      dtype='object')\n",
      "columns of YAML are: ['event_time', 'event_type', 'product_id', 'category_id', 'category_code', 'brand', 'price', 'user_id', 'user_session']\n"
     ]
    }
   ],
   "source": [
    "print(\"columns of files are:\" ,dataframe.columns)\n",
    "print(\"columns of YAML are:\" ,config_data['columns'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68939b8e",
   "metadata": {},
   "source": [
    "#### Since we have the same columns on both files, the validation passed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7747384",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a18e10",
   "metadata": {},
   "source": [
    "# 5) Write the file in pipe separated text file (|) in gz format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e018f9b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\000.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\001.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\002.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\003.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\004.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\005.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\006.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\007.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\008.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\009.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\010.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\011.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\012.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\013.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\014.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\015.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\016.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\017.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\018.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\019.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\020.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\021.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\022.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\023.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\024.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\025.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\026.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\027.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\028.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\029.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\030.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\031.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\032.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\033.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\034.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\035.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\036.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\037.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\038.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\039.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\040.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\041.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\042.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\043.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\044.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\045.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\046.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\047.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\048.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\049.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\050.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\051.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\052.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\053.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\054.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\055.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\056.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\057.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\058.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\059.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\060.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\061.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\062.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\063.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\064.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\065.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\066.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\067.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\068.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\069.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\070.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\071.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\072.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\073.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\074.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\075.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\076.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\077.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\078.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\079.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\080.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\081.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\082.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\083.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\084.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\085.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\086.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\087.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\088.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\089.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\090.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\091.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\092.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\093.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\094.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\095.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\096.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\097.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\098.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\099.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\100.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\101.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\102.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\103.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\104.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\105.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\106.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\107.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\108.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\109.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\110.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\111.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\112.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\113.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\114.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\115.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\116.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\117.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\118.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\119.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\120.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\121.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\122.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\123.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\124.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\125.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\126.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\127.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\128.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\129.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\130.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\131.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\132.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\133.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\134.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\135.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\136.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\137.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\138.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\139.part',\n",
       " 'C:/Users/HP/Data Glacier/week 6/data.csv.gz\\\\140.part']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import csv\n",
    "import gzip\n",
    "\n",
    "from dask import dataframe as dd\n",
    "df = dd.read_csv('data.csv',delimiter=',')\n",
    "\n",
    "# Write csv in gz format in pipe separated text file (|)\n",
    "df.to_csv('data.csv.gz',\n",
    "          sep='|',\n",
    "          header=True,\n",
    "          index=False,\n",
    "          quoting=csv.QUOTE_ALL,\n",
    "          compression='gzip',\n",
    "          quotechar='\"',\n",
    "          doublequote=True,\n",
    "          line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09641a06",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9410c96",
   "metadata": {},
   "source": [
    "# 6) Create a summary of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "be69e0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of rows:  67501979\n",
      "The number of columns:  9\n"
     ]
    }
   ],
   "source": [
    "#Total number of rows of the file: \n",
    "print(\"The total number of rows: \", len(dataframe))\n",
    "#Total number of columns of the file:\n",
    "print(\"The number of columns: \", len(dataframe.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7f737371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of rows:  4\n",
      "The number of columns:  9\n"
     ]
    }
   ],
   "source": [
    "#Total number of rows of the YAML: \n",
    "print(\"The total number of rows: \", len(config_data))\n",
    "#Total number of columns of the YAML:\n",
    "print(\"The number of columns: \", len(config_data['columns']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0aa54f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Size is : 9006762395 bytes\n"
     ]
    }
   ],
   "source": [
    "# The size of the CSV\n",
    "file_size = os.path.getsize('data.csv')\n",
    "print(\"File Size is :\", file_size, \"bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad314f19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
